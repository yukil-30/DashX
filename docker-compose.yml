version: '3.8'

services:
  postgres:
    image: postgres:15-alpine
    container_name: restaurant_postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-restaurant_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-restaurant_password}
      POSTGRES_DB: ${POSTGRES_DB:-restaurant_db}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-restaurant_user} -d ${POSTGRES_DB:-restaurant_db}"]
      interval: 5s
      timeout: 5s
      retries: 5

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: restaurant_backend
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgresql://restaurant_user:restaurant_password@postgres:5432/restaurant_db}
      LLM_STUB_URL: ${LLM_STUB_URL:-http://llm-stub:8001}
      LOCAL_LLM_URL: ${LOCAL_LLM_URL:-http://local-llm:8080}
      LLM_ADAPTER: ${LLM_ADAPTER:-stub}
      ENABLE_LOCAL_LLM: ${ENABLE_LOCAL_LLM:-false}
      DEBUG: ${DEBUG:-true}
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      llm-stub:
        condition: service_started
    volumes:
      - ./backend:/app
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: restaurant_frontend
    environment:
      VITE_API_URL: ${VITE_API_URL:-http://localhost:8000}
    ports:
      - "3000:3000"
    depends_on:
      - backend
    volumes:
      - ./frontend:/app
      - /app/node_modules

  llm-stub:
    build:
      context: ./llm-stub
      dockerfile: Dockerfile
    container_name: restaurant_llm_stub
    ports:
      - "8001:8001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  # Local LLM Service - Runs a small language model locally
  # 
  # Setup:
  #   1. Download a model: ./scripts/download_model.sh
  #   2. Enable service: ENABLE_LOCAL_LLM=true docker compose up local-llm
  #
  # To use a different model:
  #   - Place your GGUF model at ./models/local-llm/model.gguf
  #   - Or set MODEL_URL before running download script
  #
  # For stub mode (no model, canned responses):
  #   STUB_MODE=true docker compose up local-llm
  #
  local-llm:
    build:
      context: ./local-llm
      dockerfile: Dockerfile
    container_name: restaurant_local_llm
    environment:
      MODEL_PATH: /models/model.gguf
      N_CTX: ${LOCAL_LLM_CTX:-2048}
      N_THREADS: ${LOCAL_LLM_THREADS:-4}
      N_GPU_LAYERS: ${LOCAL_LLM_GPU_LAYERS:-0}
      STUB_MODE: ${LOCAL_LLM_STUB_MODE:-false}
    ports:
      - "8080:8080"
    volumes:
      - ./models/local-llm:/models:ro
    deploy:
      resources:
        limits:
          cpus: "${LOCAL_LLM_CPUS:-1.0}"
          memory: "${LOCAL_LLM_MEMORY:-2g}"
        reservations:
          memory: "512m"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      start_period: 60s
      retries: 3
    profiles:
      - local-llm  # Only start when explicitly requested or ENABLE_LOCAL_LLM=true

volumes:
  postgres_data:
